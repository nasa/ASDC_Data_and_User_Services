{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e8fbf5",
   "metadata": {},
   "source": [
    "---\n",
    "date: last-modified\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fb369-0882-45de-a790-d3584565d217",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exploring aerosol data from DSCOVR EPIC Level 2 Aerosol Version 3\n",
    "\n",
    "## Summary\n",
    "\n",
    "We investigate the time lag between wildfire smoke detected by satellite scans and the corresponding readings from ground-based sensors. This analysis is crucial for understanding the behavior and transport of wildfire smoke plumes as they descend from the atmosphere to ground level, impacting air quality. We utilize satellite-derived Aerosol Index (AI) data, which indicates the presence of aerosols in the atmosphere, and ground-based Particulate Matter (PM) measurements, particularly PM2.5, to quantify smoke concentrations at the surface.\n",
    "To analyze the time lag, we first normalize the AI units from satellite data and PM2.5 units from the ground sensors to create comparable time series. By shifting the ground sensor data over a range of time intervals, we aim to identify the optimal lag where the ground-based PM2.5 readings align with peaks in the satellite-derived AI values. This approach allows us to estimate the delay between smoke detection in the upper atmosphere and its eventual impact on air quality at the surface. Various statistical methods, such as cross-correlation, are employed to quantify this time lag and improve the predictive understanding of smoke dispersion patterns.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "\"DSCOVR_EPIC_L2_AER_03 is the Deep Space Climate Observatory (DSCOVR) Enhanced Polychromatic Imaging Camera (EPIC) Level 2 UV Aerosol Version 3 data product. Observations for this data product are at 340 and 388 nm and are used to derive near UV (ultraviolet) aerosol properties. The EPIC aerosol retrieval algorithm (EPICAERUV) uses a set of aerosol models to account for the presence of carbonaceous aerosols from biomass burning and wildfires (BIO), desert dust (DST), and sulfate-based (SLF) aerosols. These aerosol models are identical to those assumed in the OMI (Ozone Monitoring Instrument) algorithm (Torres et al., 2007; Jethva and Torres, 2011).\" ([Source](https://asdc.larc.nasa.gov/project/DSCOVR/DSCOVR_EPIC_L2_AER_03))\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook requires these libraries and files:\n",
    "- [cartopy](https://scitools.org.uk/cartopy/docs/latest/)\n",
    "- [earthaccess](https://earthaccess.readthedocs.io/en/latest/)\n",
    "- [netCDF4-python](https://unidata.github.io/netcdf4-python/)\n",
    "- [numpy](https://numpy.org/)\n",
    "- [matplotlib](http://matplotlib.org/)\n",
    "- [pyrsig](https://barronh.github.io/pyrsig/)\n",
    "- [scipy](https://scipy.org/)\n",
    "- [tqdm](https://tqdm.github.io/)\n",
    "- [xarray](https://docs.xarray.dev/en/stable/)\n",
    "- Data granules from [DSCOVR EPIC Level-2](https://cmr.earthdata.nasa.gov/search/concepts/C1962643459-LARC_ASDC.html) to have been downloaded into a local directory.\n",
    "\n",
    "### Notebook Author / Affiliation\n",
    "\n",
    "- Created: 2023\n",
    "- Authors: Jackson Baeza and Hazem Mahmoud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebcc26-b07d-42a6-beb4-2bb58d771071",
   "metadata": {},
   "source": [
    "# Section 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813d73c-0f9d-48fc-8ae1-2986faba579c",
   "metadata": {},
   "source": [
    "## 1. Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e09cc2-dd86-4ebc-8f92-b1830c8cfd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import earthaccess\n",
    "\n",
    "# For the timeseries section\n",
    "from scipy import spatial\n",
    "\n",
    "# For the timeseries section, to compare with EPA Air Quality data\n",
    "import pyrsig\n",
    "\n",
    "# Fot the animation section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43e044-28b3-44ff-a7eb-7a854bea4306",
   "metadata": {},
   "source": [
    "## 2. Download data files\n",
    "\n",
    "This section defines the time, and location to pull the data files from. After running once there is no reason to run this again. \n",
    "Also this section can be altered to change the type of data I am pulling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2188b449-4437-4733-ad8a-7f1751b06346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data/dscovr/\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12cff95-833a-4df7-8f1b-9f930bfe7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6f9fe7-ee4a-4046-9eca-5d868af6f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"C1962643459-LARC_ASDC\"\n",
    "\n",
    "# Bounds within which we search for data granules\n",
    "date_start = \"2023-06-02 00:00\"\n",
    "date_end = \"2023-06-15 23:59\"\n",
    "date_range = (date_start, date_end)\n",
    "bbox = (-79, 34, -73, 40)  # minLon, minLat, maxLon, maxLat\n",
    "# extent = [-79, -73, 35, 39.5]               # extend = [minLon, maxLon, minLat, maxLat]\n",
    "\n",
    "# Find the data granules and their links\n",
    "results = earthaccess.search_data(\n",
    "    concept_id=collection_id,\n",
    "    temporal=date_range,\n",
    "    bounding_box=bbox,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974e6a8-904a-43eb-8c1f-d6bd48c76e12",
   "metadata": {},
   "source": [
    "**!!! Warning !!! This next cell can take a long time.  It needs to be run once, but then should be commented-out if the data files are already in a local directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a44aa5-dbf0-4fe9-9704-e8328c4f0576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f4d9c3ddcc4d8d90962b7021a1e53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6ef06d47834c3ba7e5895478838673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132b0cd13b774a2da3f8d46e52aaa488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the files\n",
    "downloaded_files = earthaccess.download(\n",
    "    results,\n",
    "    local_path=data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659303ca-a17c-4d8d-b748-f692913aa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the list of files that are present\n",
    "pattern = re.compile(\n",
    "    r\"DSCOVR_EPIC_L2_AER_03_([0-9]+)_03\"\n",
    ")  # the capture group is a datetime timestamp\n",
    "\n",
    "file_list = list(data_dir.glob(\"DSCOVR_EPIC_L2_AER_*.he5\"))\n",
    "num_files = len(file_list)\n",
    "\n",
    "# These are the paths to groups *within* each HDF file\n",
    "data_fields_path = \"HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields\"\n",
    "geolocations_path = \"HDFEOS/SWATHS/Aerosol NearUV Swath/Geolocation Fields\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726c75d-30d9-4b25-b4df-8216a5dc7eda",
   "metadata": {},
   "source": [
    "## 3. Wrangling data into Zarr stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995ccf7-83f3-425a-80f9-2070b9d1ea8d",
   "metadata": {},
   "source": [
    "In this section, the HDF files are converted to Zarr stores to enable faster access for data manipulation later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2703b6-f24d-451a-a9ca-bcfe80e7cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hdf_to_xrds(filepath: Path, group_path: str) -> xr.Dataset:\n",
    "    \"\"\"Converts a HDF group into an xarray Dataset with an added time dimension\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : pathlib.Path\n",
    "        The path to a DSCOVR EPIC HDF file\n",
    "    group_path : str\n",
    "        The internal path to a group, e.g., 'HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields'\n",
    "    \"\"\"\n",
    "    # The filename's datetime string is converted into a datetime timestamp\n",
    "    timestamp = pattern.findall(str(fp.stem))[0]\n",
    "    timestamp_dt = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Note: the netCDF library works to access these datasets, so we just use it instead of h5py.\n",
    "    with nc.Dataset(filepath) as ds:\n",
    "        grp = ds[group_path]\n",
    "\n",
    "        # The HDF group is converted into an xarray Dataset object, and then\n",
    "        #     a new singleton 'time' dimension is added to the Dataset with the timestamp as its only value.\n",
    "        grp_ds = xr.open_dataset(xr.backends.NetCDF4DataStore(grp)).expand_dims(\n",
    "            time=[timestamp_dt], axis=0\n",
    "        )\n",
    "\n",
    "    return grp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae9a3cb-bdec-4eca-803a-1afa8557ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nc_to_xrds(filepath: Path, group_path: str) -> xr.Dataset:\n",
    "    \"\"\"Converts a HDF group into an xarray Dataset with an added time dimension\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : pathlib.Path\n",
    "        The path to a DSCOVR EPIC HDF file\n",
    "    group_path : str\n",
    "        The internal path to a group, e.g., 'HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields'\n",
    "    \"\"\"\n",
    "    # The filename's datetime string is converted into a datetime timestamp\n",
    "    timestamp = pattern.findall(str(fp.stem))[0]\n",
    "    timestamp_dt = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Note: the netCDF library works to access these datasets, so we just use it instead of h5py.\n",
    "    with nc.Dataset(filepath) as ds:\n",
    "        grp = ds[group_path]\n",
    "\n",
    "        # The HDF group is converted into an xarray Dataset object, and then\n",
    "        #     a new singleton 'time' dimension is added to the Dataset with the timestamp as its only value.\n",
    "        grp_ds = xr.open_dataset(xr.backends.NetCDF4DataStore(grp)).expand_dims(\n",
    "            time=[timestamp_dt], axis=0\n",
    "        )\n",
    "\n",
    "    return grp_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971b2fd-1d2b-4a24-b70a-7fdacdd45fac",
   "metadata": {},
   "source": [
    "**!!! Warning !!! This next cell can take a long time.  It needs to be run once, but then should be commented-out if the zarr files already exist.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6def0502-b9a1-4af8-8c94-3e6c49e21b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting groups...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e67908b37145d283b07f7524a93e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ContainsGroupError",
     "evalue": "path '' contains a group",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mContainsGroupError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(idcs_to_do, total=\u001b[38;5;28mlen\u001b[39m(idcs_to_do)):\n\u001b[32m      5\u001b[39m     fp = file_list[idx]\n\u001b[32m      6\u001b[39m     \u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_hdf_to_xrds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_fields_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m         \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mXDim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYDim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnLayers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnWave3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnWave2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_nc_to_xrds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeolocations_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m          \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mXDim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                   \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphony_dim_6\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYDim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzStore01/zarr_2024_07_09_#\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m03\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.zarr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m#if idx > 2:\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m#    break\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone extracting groups.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/core/dataset.py:2270\u001b[39m, in \u001b[36mDataset.to_zarr\u001b[39m\u001b[34m(self, store, chunk_store, mode, synchronizer, group, encoding, compute, consolidated, append_dim, region, safe_chunks, storage_options, zarr_version, zarr_format, write_empty_chunks, chunkmanager_store_kwargs)\u001b[39m\n\u001b[32m   2102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Write dataset contents to a zarr group.\u001b[39;00m\n\u001b[32m   2103\u001b[39m \n\u001b[32m   2104\u001b[39m \u001b[33;03mZarr chunks are determined in the following way:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2266\u001b[39m \u001b[33;03m    The I/O user guide, with more details and examples.\u001b[39;00m\n\u001b[32m   2267\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_zarr\n\u001b[32m-> \u001b[39m\u001b[32m2270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload,misc]\u001b[39;49;00m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2276\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2283\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafe_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_empty_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_empty_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunkmanager_store_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunkmanager_store_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/backends/api.py:2217\u001b[39m, in \u001b[36mto_zarr\u001b[39m\u001b[34m(dataset, store, chunk_store, mode, synchronizer, group, encoding, compute, consolidated, append_dim, region, safe_chunks, storage_options, zarr_version, zarr_format, write_empty_chunks, chunkmanager_store_kwargs)\u001b[39m\n\u001b[32m   2214\u001b[39m     already_consolidated = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2215\u001b[39m     consolidate_on_close = consolidated \u001b[38;5;129;01mor\u001b[39;00m consolidated \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2217\u001b[39m zstore = \u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZarrStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2220\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43malready_consolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_region\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafe_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_empty_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2234\u001b[39m dataset = zstore._validate_and_autodetect_region(dataset)\n\u001b[32m   2235\u001b[39m zstore._validate_encoding(encoding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:732\u001b[39m, in \u001b[36mZarrStore.open_group\u001b[39m\u001b[34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, zarr_version, zarr_format, use_zarr_fill_value_as_mask, write_empty, cache_members)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_group\u001b[39m(\n\u001b[32m    709\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    725\u001b[39m     cache_members: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    726\u001b[39m ):\n\u001b[32m    727\u001b[39m     (\n\u001b[32m    728\u001b[39m         zarr_group,\n\u001b[32m    729\u001b[39m         consolidate_on_close,\n\u001b[32m    730\u001b[39m         close_store_on_close,\n\u001b[32m    731\u001b[39m         use_zarr_fill_value_as_mask,\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     ) = \u001b[43m_get_open_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    747\u001b[39m         zarr_group,\n\u001b[32m    748\u001b[39m         mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m         cache_members,\n\u001b[32m    757\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/backends/zarr.py:1845\u001b[39m, in \u001b[36m_get_open_params\u001b[39m\u001b[34m(store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, zarr_version, use_zarr_fill_value_as_mask, zarr_format)\u001b[39m\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _zarr_v3():\n\u001b[32m   1842\u001b[39m         \u001b[38;5;66;03m# we have determined that we don't want to use consolidated metadata\u001b[39;00m\n\u001b[32m   1843\u001b[39m         \u001b[38;5;66;03m# so we set that to False to avoid trying to read it\u001b[39;00m\n\u001b[32m   1844\u001b[39m         open_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_consolidated\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m     zarr_group = \u001b[43mzarr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1847\u001b[39m close_store_on_close = zarr_group.store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m store\n\u001b[32m   1849\u001b[39m \u001b[38;5;66;03m# we use this to determine how to handle fill_value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/zarr/hierarchy.py:1593\u001b[39m, in \u001b[36mopen_group\u001b[39m\u001b[34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, meta_array)\u001b[39m\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContainsArrayError(path)\n\u001b[32m   1592\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m contains_group(store, path=path):\n\u001b[32m-> \u001b[39m\u001b[32m1593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContainsGroupError(path)\n\u001b[32m   1594\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1595\u001b[39m     init_group(store, path=path, chunk_store=chunk_store)\n",
      "\u001b[31mContainsGroupError\u001b[39m: path '' contains a group"
     ]
    }
   ],
   "source": [
    "print(\"Extracting groups...\")\n",
    "\n",
    "idcs_to_do = range(0, len(file_list))\n",
    "for idx in tqdm(idcs_to_do, total=len(idcs_to_do)):\n",
    "    fp = file_list[idx]\n",
    "    (\n",
    "        convert_hdf_to_xrds(fp, data_fields_path).rename(\n",
    "            {\n",
    "                \"phony_dim_0\": \"XDim\",\n",
    "                \"phony_dim_1\": \"YDim\",\n",
    "                \"phony_dim_2\": \"nLayers\",\n",
    "                \"phony_dim_3\": \"nWave3\",\n",
    "                \"phony_dim_4\": \"nWave2\",\n",
    "            }\n",
    "        )\n",
    "    ).merge(\n",
    "        (\n",
    "            convert_nc_to_xrds(fp, geolocations_path).rename(\n",
    "                {\"phony_dim_5\": \"XDim\", \"phony_dim_6\": \"YDim\"}\n",
    "            )\n",
    "        )\n",
    "    ).to_zarr(f\"zStore01/zarr_2024_07_09_#{idx:03}.zarr\")\n",
    "\n",
    "    # if idx > 2:\n",
    "    #    break\n",
    "print(\"Done extracting groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cbef1-3c7b-4be8-ba69-e8b3f554c583",
   "metadata": {},
   "source": [
    "## 4. Now Open the dataset\n",
    "\n",
    "This will load the Zarr files from the local saved, and then look at some of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fae3e597-06fd-4df2-9d69-7c5a99795107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfds = xr.open_mfdataset(glob(\"zStore01/zarr_2024_07_09_#*.zarr\"),\n",
    "#                          engine='zarr', combine='by_coords')\n",
    "# mfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75be7c86-ee9c-4d2c-9ccf-51579bd98d6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find any dimension coordinates to use to order the datasets for concatenation",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mfds = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/dscovr/DSCOVR*.he5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnetcdf4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mby_coords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m mfds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/backends/api.py:1663\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[39m\n\u001b[32m   1650\u001b[39m     combined = _nested_combine(\n\u001b[32m   1651\u001b[39m         datasets,\n\u001b[32m   1652\u001b[39m         concat_dims=concat_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1658\u001b[39m         combine_attrs=combine_attrs,\n\u001b[32m   1659\u001b[39m     )\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m combine == \u001b[33m\"\u001b[39m\u001b[33mby_coords\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1661\u001b[39m     \u001b[38;5;66;03m# Redo ordering from coordinates, ignoring how they were ordered\u001b[39;00m\n\u001b[32m   1662\u001b[39m     \u001b[38;5;66;03m# previously\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m     combined = \u001b[43mcombine_by_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1672\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1673\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombine\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is an invalid option for the keyword argument ``combine``\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/structure/combine.py:983\u001b[39m, in \u001b[36mcombine_by_coords\u001b[39m\u001b[34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[39m\n\u001b[32m    979\u001b[39m     grouped_by_vars = groupby_defaultdict(data_objects, key=vars_as_keys)\n\u001b[32m    981\u001b[39m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[32m    982\u001b[39m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     concatenated_grouped_by_data_vars = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrouped_by_vars\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[32m    997\u001b[39m     concatenated_grouped_by_data_vars,\n\u001b[32m    998\u001b[39m     compat=compat,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1001\u001b[39m     combine_attrs=combine_attrs,\n\u001b[32m   1002\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/structure/combine.py:984\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    979\u001b[39m     grouped_by_vars = groupby_defaultdict(data_objects, key=vars_as_keys)\n\u001b[32m    981\u001b[39m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[32m    982\u001b[39m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[32m    983\u001b[39m     concatenated_grouped_by_data_vars = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m         \u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, datasets_with_same_vars \u001b[38;5;129;01min\u001b[39;00m grouped_by_vars\n\u001b[32m    994\u001b[39m     )\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[32m    997\u001b[39m     concatenated_grouped_by_data_vars,\n\u001b[32m    998\u001b[39m     compat=compat,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1001\u001b[39m     combine_attrs=combine_attrs,\n\u001b[32m   1002\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/structure/combine.py:645\u001b[39m, in \u001b[36m_combine_single_variable_hypercube\u001b[39m\u001b[34m(datasets, fill_value, data_vars, coords, compat, join, combine_attrs)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(datasets) == \u001b[32m0\u001b[39m:\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    641\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAt least one Dataset is required to resolve variable names \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    642\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor combined hypercube.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    643\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m combined_ids, concat_dims = \u001b[43m_infer_concat_order_from_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    648\u001b[39m     \u001b[38;5;66;03m# check that datasets form complete hypercube\u001b[39;00m\n\u001b[32m    649\u001b[39m     _check_shape_tile_ids(combined_ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/temp-dscovr-baeza-notebook/lib/python3.12/site-packages/xarray/structure/combine.py:158\u001b[39m, in \u001b[36m_infer_concat_order_from_coords\u001b[39m\u001b[34m(datasets)\u001b[39m\n\u001b[32m    152\u001b[39m             tile_ids = [\n\u001b[32m    153\u001b[39m                 tile_id + (position,)\n\u001b[32m    154\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m tile_id, position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tile_ids, order, strict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    155\u001b[39m             ]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(datasets) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m concat_dims:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not find any dimension coordinates to use to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33morder the datasets for concatenation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m     )\n\u001b[32m    163\u001b[39m combined_ids = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tile_ids, datasets, strict=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combined_ids, concat_dims\n",
      "\u001b[31mValueError\u001b[39m: Could not find any dimension coordinates to use to order the datasets for concatenation"
     ]
    }
   ],
   "source": [
    "mfds = xr.open_mfdataset(glob(\"data/dscovr/DSCOVR*.he5\"), engine=\"netcdf4\", combine=\"by_coords\")\n",
    "mfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb2c7b-8f87-49fe-9737-79d59b0ab4dd",
   "metadata": {},
   "source": [
    "## 5. Creating ColorMaps\n",
    "\n",
    "In the Original they built off of the Turbo color pallatte, but here I have defined four more  color pallates that are colorblind inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2d29d-09aa-4669-8547-f559b64b8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neccesary Imports\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15b4a8-1010-4c52-9a90-75512782cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muted ColorMap\n",
    "muted_cmap_name = \"muted_cmap\"\n",
    "muted_qualitative_colors = [\n",
    "    # (51/256, 34/256, 136/256),   # Indigo\n",
    "    # (136/256, 204/256, 238/256), # Cyan\n",
    "    (68 / 256, 170 / 256, 153 / 256),  # Teal\n",
    "    (17 / 256, 119 / 256, 51 / 256),  # Green\n",
    "    (153 / 256, 153 / 256, 51 / 256),  # Olive\n",
    "    (221 / 256, 204 / 256, 119 / 256),  # Sand\n",
    "    (204 / 256, 102 / 256, 119 / 256),  # Rose\n",
    "    (136 / 256, 34 / 256, 85 / 256),  # Wine\n",
    "    (170 / 256, 68 / 256, 153 / 256),\n",
    "]  # Purple\n",
    "\n",
    "muted_cm = LinearSegmentedColormap.from_list(muted_cmap_name, muted_qualitative_colors)\n",
    "newcolors = muted_cm(np.linspace(0, 1, 256))\n",
    "silver_color = np.array([0.95, 0.95, 0.95, 1])\n",
    "newcolors[0, :] = silver_color\n",
    "muted_cmap = ListedColormap(newcolors)\n",
    "muted_cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29b6ff-d4aa-48bd-9c08-1dbf5601bffe",
   "metadata": {},
   "source": [
    "## 6. Define Mapping Functions, Define total Extent, and Choose Data Variable\n",
    "\n",
    "This section defines the Extent for the later graphs, declares the data variable to look at, and then defines two utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656b49d-b5dc-4579-bd7e-2a957daee50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility function will simplify some code blocks later.\n",
    "def get_geo_mask(lon, lat):\n",
    "    lon_mask = (lon > extent[0]) & (lon < extent[1])\n",
    "    lat_mask = (lat > extent[2]) & (lat < extent[3])\n",
    "    return lon_mask & lat_mask\n",
    "\n",
    "\n",
    "# This utility function will simplify some code blocks later.\n",
    "def get_data_arrays(a_timestep: int):\n",
    "    geo_mask = (\n",
    "        get_geo_mask(mfds[\"Longitude\"][a_timestep, :, :], mfds[\"Latitude\"][a_timestep, :, :])\n",
    "        .compute()\n",
    "        .values\n",
    "    )\n",
    "    lon_values = mfds[\"Longitude\"][a_timestep, :, :].where(geo_mask).values\n",
    "    lat_values = mfds[\"Latitude\"][a_timestep, :, :].where(geo_mask).values\n",
    "    var_values = mfds[data_variable][a_timestep, :, :].where(geo_mask).values\n",
    "    timestamp = datetime.strptime(\n",
    "        np.datetime_as_string(mfds[\"time\"][a_timestep].values, unit=\"ms\"), \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "    )\n",
    "\n",
    "    # Replace any +inf or -inf values with NaN.\n",
    "    var_values[np.isinf(var_values)] = np.nan\n",
    "\n",
    "    return lon_values, lat_values, var_values, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ab89d-0e11-40fd-9a71-9d5559807d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines Extent for greater specificatoin\n",
    "extent = [-79, -73, 35, 39.5]  # extend = [minLon, maxLon, minLat, maxLat]\n",
    "central_lon = np.mean(extent[:2])\n",
    "central_lat = np.mean(extent[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c071c-25ae-493c-bbc5-6f60e79989e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the desired Data Variable\n",
    "data_variable = \"UVAerosolIndex\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3d35d-198f-4d15-8515-d8cd31bf77c1",
   "metadata": {},
   "source": [
    "## 7. Make A Before and During Timestep UVAI Map\n",
    "\n",
    "This section creates timestep maps for before and during the peak of the smoke extent in hampton roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60d712-3ad0-4dae-864e-e2d3f8b37f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country boundaries\n",
    "country_bodr = cfeature.NaturalEarthFeature(\n",
    "    category=\"cultural\",\n",
    "    name=\"admin_0_boundary_lines_land\",\n",
    "    scale=\"50m\",\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "\n",
    "# province boundaries\n",
    "provinc_bodr = cfeature.NaturalEarthFeature(\n",
    "    category=\"cultural\",\n",
    "    name=\"admin_1_states_provinces_lines\",\n",
    "    scale=\"50m\",\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"k\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23dfef0-d9ea-426a-9c66-27b87e52b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the timestep here. Timestep 30 shows clean air before the smoke arrived.\n",
    "timestep01 = 30\n",
    "\n",
    "# Get value range for the single timestep\n",
    "values_min = 0\n",
    "values_max = 10\n",
    "\n",
    "# Get the data arrays for the selected timestep\n",
    "geo_mask = (\n",
    "    get_geo_mask(mfds[\"Longitude\"][timestep01, :, :], mfds[\"Latitude\"][timestep01, :, :])\n",
    "    .compute()\n",
    "    .values\n",
    ")\n",
    "\n",
    "lon_values = mfds[\"Longitude\"][timestep01, :, :].where(geo_mask).values\n",
    "lat_values = mfds[\"Latitude\"][timestep01, :, :].where(geo_mask).values\n",
    "var_values = mfds[data_variable][timestep01, :, :].where(geo_mask).values\n",
    "timestamp = datetime.strptime(\n",
    "    np.datetime_as_string(mfds[\"time\"][timestep01].values, unit=\"ms\"), \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    ")\n",
    "\n",
    "# Encoding the actual Plot\n",
    "my_projection = ccrs.PlateCarree()\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(12, 9), subplot_kw={\"projection\": my_projection}\n",
    ")  # , \"extent\": extent})\n",
    "\n",
    "vmin, vmax = values_min, values_max\n",
    "levels = 50\n",
    "level_boundaries = np.linspace(vmin, vmax, levels + 1)\n",
    "\n",
    "_contourf = ax.contourf(\n",
    "    lon_values,\n",
    "    lat_values,\n",
    "    var_values,\n",
    "    cmap=muted_cmap,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    levels=level_boundaries,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    extend=\"both\",\n",
    ")\n",
    "\n",
    "ax.add_feature(cfeature.STATES, linestyle=\"--\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "cb_handle = plt.colorbar(\n",
    "    _contourf,\n",
    "    orientation=\"horizontal\",\n",
    "    ticks=range(int(np.floor(vmin)), int(np.ceil(vmax + 1)), 1),\n",
    "    boundaries=level_boundaries,\n",
    "    values=(level_boundaries[:-1] + level_boundaries[1:]) / 2,\n",
    "    ax=ax,\n",
    ")\n",
    "cb_handle.ax.set_title(data_variable, fontsize=18)\n",
    "\n",
    "ax.set_title(\"%s\" % (timestamp.strftime(\"%Y%m%d %H:%M:%S\")), fontsize=18)\n",
    "\n",
    "gl_handle = ax.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=[\"left\", \"bottom\"],\n",
    "    linewidth=0.8,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\":\",\n",
    ")\n",
    "\n",
    "# We change the fontsize tick labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "cb_handle.ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "gl_handle.xlabel_style, gl_handle.ylabel_style = {\"fontsize\": 16}, {\"fontsize\": 16}\n",
    "\n",
    "\n",
    "plt.savefig(\"UVAerosolIndex/June05\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb38ac7-0b48-4318-bd07-f60258cf209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the timestep here. Timestep 48 shows dangerous levels of air quality as the smoke moved into the area.\n",
    "timestep02 = 48\n",
    "\n",
    "# Get value range for the single timestep\n",
    "values_min = 0\n",
    "values_max = 10\n",
    "\n",
    "# Get the data arrays for the selected timestep\n",
    "geo_mask = (\n",
    "    get_geo_mask(mfds[\"Longitude\"][timestep02, :, :], mfds[\"Latitude\"][timestep02, :, :])\n",
    "    .compute()\n",
    "    .values\n",
    ")\n",
    "\n",
    "lon_values = mfds[\"Longitude\"][timestep02, :, :].where(geo_mask).values\n",
    "lat_values = mfds[\"Latitude\"][timestep02, :, :].where(geo_mask).values\n",
    "var_values = mfds[data_variable][timestep02, :, :].where(geo_mask).values\n",
    "timestamp = datetime.strptime(\n",
    "    np.datetime_as_string(mfds[\"time\"][timestep02].values, unit=\"ms\"), \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    ")\n",
    "\n",
    "# Encoding the actual Plot\n",
    "my_projection = ccrs.PlateCarree()\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(12, 9), subplot_kw={\"projection\": my_projection}\n",
    ")  # , \"extent\": extent})\n",
    "\n",
    "vmin, vmax = values_min, values_max\n",
    "levels = 50\n",
    "level_boundaries = np.linspace(vmin, vmax, levels + 1)\n",
    "\n",
    "_contourf = ax.contourf(\n",
    "    lon_values,\n",
    "    lat_values,\n",
    "    var_values,\n",
    "    cmap=muted_cmap,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    levels=level_boundaries,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    extend=\"both\",\n",
    ")\n",
    "\n",
    "ax.add_feature(cfeature.STATES, linestyle=\"--\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "cb_handle = plt.colorbar(\n",
    "    _contourf,\n",
    "    orientation=\"horizontal\",\n",
    "    ticks=range(int(np.floor(vmin)), int(np.ceil(vmax + 1)), 1),\n",
    "    boundaries=level_boundaries,\n",
    "    values=(level_boundaries[:-1] + level_boundaries[1:]) / 2,\n",
    "    ax=ax,\n",
    ")\n",
    "cb_handle.ax.set_title(data_variable, fontsize=18)\n",
    "\n",
    "ax.set_title(\"%s\" % (timestamp.strftime(\"%Y%m%d %H:%M:%S\")), fontsize=18)\n",
    "\n",
    "gl_handle = ax.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=[\"left\", \"bottom\"],\n",
    "    linewidth=0.8,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\":\",\n",
    ")\n",
    "\n",
    "# We change the fontsize tick labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "cb_handle.ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "gl_handle.xlabel_style, gl_handle.ylabel_style = {\"fontsize\": 16}, {\"fontsize\": 16}\n",
    "\n",
    "\n",
    "# plt.savefig(\"UVAerosolIndex/June06\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bff662-d64b-460e-b8bf-c03aa28a8dc1",
   "metadata": {},
   "source": [
    "## 8. Change Data Variable, Create graphs for Final Aerosol Layer height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459c3a5-5299-45cc-8e39-cee43a42f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the desired Data Variable\n",
    "data_variable = \"FinalAerosolLayerHeight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac65ce-9e01-43a6-9e88-090c22aee657",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_mask = (\n",
    "    get_geo_mask(mfds[\"Longitude\"][timestep01, :, :], mfds[\"Latitude\"][timestep01, :, :])\n",
    "    .compute()\n",
    "    .values\n",
    ")\n",
    "\n",
    "lon_values = mfds[\"Longitude\"][timestep01, :, :].where(geo_mask).values\n",
    "lat_values = mfds[\"Latitude\"][timestep01, :, :].where(geo_mask).values\n",
    "var_values = mfds[data_variable][timestep01, :, :].where(geo_mask).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd11786-21c5-4e92-a6d7-3e98d6df5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the timestep here. Timestep 30 shows clean air before the smoke arrived.\n",
    "timestep01 = 48\n",
    "\n",
    "# Get value range for the single timestep\n",
    "values_min = 0\n",
    "values_max = 10\n",
    "\n",
    "# Get the data arrays for the selected timestep\n",
    "geo_mask = (\n",
    "    get_geo_mask(mfds[\"Longitude\"][timestep01, :, :], mfds[\"Latitude\"][timestep01, :, :])\n",
    "    .compute()\n",
    "    .values\n",
    ")\n",
    "\n",
    "lon_values = mfds[\"Longitude\"][timestep01, :, :].where(geo_mask).values\n",
    "lat_values = mfds[\"Latitude\"][timestep01, :, :].where(geo_mask).values\n",
    "var_values = mfds[data_variable][timestep01, :, :].where(geo_mask).values\n",
    "timestamp = datetime.strptime(\n",
    "    np.datetime_as_string(mfds[\"time\"][timestep01].values, unit=\"ms\"), \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    ")\n",
    "\n",
    "# Encoding the actual Plot\n",
    "my_projection = ccrs.PlateCarree()\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(12, 9), subplot_kw={\"projection\": my_projection}\n",
    ")  # , \"extent\": extent})\n",
    "\n",
    "vmin, vmax = values_min, values_max\n",
    "levels = 50\n",
    "level_boundaries = np.linspace(vmin, vmax, levels + 1)\n",
    "\n",
    "_contourf = ax.contourf(\n",
    "    lon_values,\n",
    "    lat_values,\n",
    "    var_values,\n",
    "    cmap=muted_cmap,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    levels=level_boundaries,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    extend=\"both\",\n",
    ")\n",
    "\n",
    "ax.add_feature(cfeature.STATES, linestyle=\"--\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "cb_handle = plt.colorbar(\n",
    "    _contourf,\n",
    "    orientation=\"horizontal\",\n",
    "    ticks=range(int(np.floor(vmin)), int(np.ceil(vmax + 1)), 1),\n",
    "    boundaries=level_boundaries,\n",
    "    values=(level_boundaries[:-1] + level_boundaries[1:]) / 2,\n",
    "    ax=ax,\n",
    ")\n",
    "cb_handle.ax.set_title(data_variable, fontsize=18)\n",
    "\n",
    "ax.set_title(\"%s\" % (timestamp.strftime(\"%Y%m%d %H:%M:%S\")), fontsize=18)\n",
    "\n",
    "gl_handle = ax.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=[\"left\", \"bottom\"],\n",
    "    linewidth=0.8,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\":\",\n",
    ")\n",
    "\n",
    "# We change the fontsize tick labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "cb_handle.ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "gl_handle.xlabel_style, gl_handle.ylabel_style = {\"fontsize\": 16}, {\"fontsize\": 16}\n",
    "\n",
    "\n",
    "plt.savefig(\"FinalAerosolLayerHeight/June06\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a9f6f-993c-4a78-8869-7a0da49bddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the timestep here. Timestep 30 shows clean air before the smoke arrived.\n",
    "timestep01 = 99\n",
    "\n",
    "# Get value range for the single timestep\n",
    "values_min = 0\n",
    "values_max = 10\n",
    "\n",
    "# Get the data arrays for the selected timestep\n",
    "geo_mask = (\n",
    "    get_geo_mask(mfds[\"Longitude\"][timestep01, :, :], mfds[\"Latitude\"][timestep01, :, :])\n",
    "    .compute()\n",
    "    .values\n",
    ")\n",
    "\n",
    "lon_values = mfds[\"Longitude\"][timestep01, :, :].where(geo_mask).values\n",
    "lat_values = mfds[\"Latitude\"][timestep01, :, :].where(geo_mask).values\n",
    "var_values = mfds[data_variable][timestep01, :, :].where(geo_mask).values\n",
    "timestamp = datetime.strptime(\n",
    "    np.datetime_as_string(mfds[\"time\"][timestep01].values, unit=\"ms\"), \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    ")\n",
    "\n",
    "# Encoding the actual Plot\n",
    "my_projection = ccrs.PlateCarree()\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(12, 9), subplot_kw={\"projection\": my_projection}\n",
    ")  # , \"extent\": extent})\n",
    "\n",
    "vmin, vmax = values_min, values_max\n",
    "levels = 50\n",
    "level_boundaries = np.linspace(vmin, vmax, levels + 1)\n",
    "\n",
    "_contourf = ax.contourf(\n",
    "    lon_values,\n",
    "    lat_values,\n",
    "    var_values,\n",
    "    cmap=muted_cmap,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    levels=level_boundaries,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    extend=\"both\",\n",
    ")\n",
    "\n",
    "ax.add_feature(cfeature.STATES, linestyle=\"--\", linewidth=1, edgecolor=\"w\")\n",
    "\n",
    "cb_handle = plt.colorbar(\n",
    "    _contourf,\n",
    "    orientation=\"horizontal\",\n",
    "    ticks=range(int(np.floor(vmin)), int(np.ceil(vmax + 1)), 1),\n",
    "    boundaries=level_boundaries,\n",
    "    values=(level_boundaries[:-1] + level_boundaries[1:]) / 2,\n",
    "    ax=ax,\n",
    ")\n",
    "cb_handle.ax.set_title(data_variable, fontsize=18)\n",
    "\n",
    "ax.set_title(\"%s\" % (timestamp.strftime(\"%Y%m%d %H:%M:%S\")), fontsize=18)\n",
    "\n",
    "gl_handle = ax.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=[\"left\", \"bottom\"],\n",
    "    linewidth=0.8,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\":\",\n",
    ")\n",
    "\n",
    "# We change the fontsize tick labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "cb_handle.ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "gl_handle.xlabel_style, gl_handle.ylabel_style = {\"fontsize\": 16}, {\"fontsize\": 16}\n",
    "\n",
    "\n",
    "# plt.savefig(\"FinalAerosolLayerHeight/June10\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013239a-7970-4078-9cf2-fb612198918c",
   "metadata": {},
   "source": [
    "# Section 2: AQS and EPIC Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f52b4f-6c4e-4a90-8757-ad2a01ea2cbe",
   "metadata": {},
   "source": [
    "## 1. Generate Times and n_times, Create Getter Function for Hampton Roads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf9918-5542-4a23-b4fc-18c1837d5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the desired Data Variable\n",
    "data_variable = \"UVAerosolIndex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aecea3-fadf-49db-b83f-4c1b52efdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [\n",
    "    datetime.strptime(np.datetime_as_string(t.values, unit=\"ms\"), \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    for t in mfds[\"time\"]\n",
    "]\n",
    "\n",
    "n_time = len(mfds.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed86d9-671f-42fb-91dc-0b307e98fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hampton_value(a_timestep: int, query_point_lon_lat: tuple):\n",
    "    lon_values, lat_values, var_values, timestamp = get_data_arrays(a_timestep)\n",
    "\n",
    "    lon_flat, lat_flat = lon_values.flatten(), lat_values.flatten()\n",
    "    new2d = np.zeros((len(lon_flat), 2))\n",
    "    for idx, (a, b) in enumerate(zip(lon_flat, lat_flat)):\n",
    "        if not np.isnan(a):\n",
    "            new2d[idx, :] = [a, b]\n",
    "\n",
    "    nearestpt_distance, nearestpt_index = spatial.KDTree(new2d).query(query_point_lon_lat)\n",
    "\n",
    "    nearestpt_coords = new2d[nearestpt_index, :]\n",
    "\n",
    "    return nearestpt_distance, nearestpt_index, nearestpt_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9b90e-5299-429c-a8ae-762b122960e6",
   "metadata": {},
   "source": [
    "**!!! Warning !!! This next cell can take a long time (e.g., +20 minutes).  It needs to be run once, but then should be commented-out.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705caa5f-2d2a-48a0-82a3-3b938a0e3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Hampton values\n",
    "# 36.9339° N, 76.3637° W\n",
    "pt_hampton = [-76.3637, 37]\n",
    "\n",
    "nearestpt_distances = []\n",
    "nearestpt_indices = []\n",
    "nearestpt_coords = []\n",
    "for i in tqdm(range(n_time)):\n",
    "    nearest_distance, nearest_index, nearest_coords = get_hampton_value(i, pt_hampton)\n",
    "    nearestpt_distances.append(nearest_distance)\n",
    "    nearestpt_indices.append(nearest_index)\n",
    "    nearestpt_coords.append(nearest_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bbd30-8a34-4ece-a74f-d79646371c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMax Scores to remove Outliers\n",
    "timeseries_values = []\n",
    "times = []\n",
    "\n",
    "# calculate the outliers\n",
    "max_x = max(nearestpt_distances)\n",
    "min_x = min(nearestpt_distances)\n",
    "\n",
    "for i in tqdm(range(len(nearestpt_distances))):\n",
    "    x = nearestpt_distances[i]\n",
    "    xscore = (x - min_x) / (max_x - min_x)\n",
    "    _, _, var_values, timestamp = get_data_arrays(i)\n",
    "\n",
    "    if xscore < 0.70:\n",
    "        nearestpt_var_value = var_values.flatten()[nearestpt_indices[i]]\n",
    "        if nearestpt_var_value >= 0:\n",
    "            timeseries_values.append(nearestpt_var_value)\n",
    "            times.append(timestamp)\n",
    "\n",
    "n_time = len(timeseries_values)\n",
    "ntime = len(nearestpt_distances)\n",
    "print(\"Original Number: \", ntime, \"\\nWithoutOutliers \", n_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441875a0-966f-4978-bcbb-e905d883bf1d",
   "metadata": {},
   "source": [
    "## 2. Use RSIG to get EPA Air Quality data from the same time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369082e4-2da1-478a-97bc-59d85beb8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hampton_roads_rsigapi = pyrsig.RsigApi(\n",
    "    bdate=\"2023-06-02 00\",\n",
    "    edate=\"2023-06-16 23:59:59\",\n",
    "    bbox=(-77, 36, -75, 38),  # Hampton Roads min lon, min lat, max lon, max lat\n",
    ")\n",
    "\n",
    "hampton_roads_aqsno2df = hampton_roads_rsigapi.to_dataframe(\n",
    "    \"aqs.pm25\", parse_dates=True, unit_keys=False\n",
    ")\n",
    "hr_aqs_time = hampton_roads_aqsno2df.groupby([\"time\"]).median(numeric_only=True)[\"pm25\"]\n",
    "\n",
    "hr_aqs_latitude = hampton_roads_aqsno2df[\"LATITUDE\"]\n",
    "hr_aqs_longitude = hampton_roads_aqsno2df[\"LONGITUDE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931613f-2560-4b5c-be8e-6545f5d8e99a",
   "metadata": {},
   "source": [
    "## 3. Make combined figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389899d6-a5f2-4345-8087-d8268be29d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_green = (17 / 256, 119 / 256, 51 / 256)  # Green\n",
    "color_purple = (204 / 256, 102 / 256, 119 / 256)  # Rose\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 12), nrows=2, ncols=1, sharex=True)\n",
    "\n",
    "# Plot\n",
    "axn = 0\n",
    "axs[axn].plot(\n",
    "    hr_aqs_time.index.values, hr_aqs_time.values, markersize=9, linewidth=4, color=color_purple\n",
    ")\n",
    "axs[axn].set_ylabel(\"Hampton, VA\\n(AQS pm25)\", fontsize=18, color=\"k\")\n",
    "\n",
    "axn = 1\n",
    "axs[axn].plot(times, timeseries_values, \":x\", markersize=7, color=color_green)\n",
    "axs[axn].set_ylabel(\n",
    "    f\"{data_variable} @Hampton, VA\\n(DSCOVR_EPIC_L2_AER)\", fontsize=18, color=\"black\"\n",
    ")\n",
    "\n",
    "# Change the aesthetics\n",
    "for a in axs:\n",
    "    a.axvspan(\n",
    "        datetime(2023, 6, 6, 0, 0, 0), datetime(2023, 6, 10, 0, 0, 0), alpha=0.3, color=\"pink\"\n",
    "    )\n",
    "    a.axhline(0, linestyle=\":\", alpha=0.3, color=\"gray\")\n",
    "\n",
    "    a.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "    a.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "    a.grid(visible=True, which=\"major\", axis=\"both\", color=\"lightgray\", linestyle=\":\")\n",
    "\n",
    "    a.tick_params(color=\"gray\", labelcolor=\"black\")\n",
    "    for spine in a.spines.values():\n",
    "        spine.set_edgecolor(\"gray\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"DscovrFigures/01_dscovr_epic_uvai_with_AQS_timeseries.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe7199-5811-4793-8321-2d0f6fd6060e",
   "metadata": {},
   "source": [
    "## 4. Parsing the AQS Data to minimize time difference between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63ccc4-0185-48a4-a28d-d6f9b6acc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import to_datetime\n",
    "from cartopy.io.shapereader import Reader\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def get_normalized_data(data):\n",
    "    max_pt = max(data)\n",
    "    min_pt = min(data)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        data[i] = (x - min_pt) / (max_pt - min_pt)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_date_splits(data, times):\n",
    "    before = []\n",
    "    during = []\n",
    "    after = []\n",
    "\n",
    "    day1 = datetime(2023, 6, 6, 0, 0, 0)\n",
    "    day2 = datetime(2023, 6, 10, 0, 0, 0)\n",
    "\n",
    "    for i in range(len(times)):\n",
    "        curr_day = times[i]\n",
    "        if curr_day < day1:\n",
    "            before.append(data[i])\n",
    "        elif curr_day < day2:\n",
    "            during.append(data[i])\n",
    "        else:\n",
    "            after.append(data[i])\n",
    "\n",
    "    return before, during, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bba15e-9ed3-4356-a16f-73f2c9f9f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because there are more points from the EPA I am going to remove some\n",
    "aqs_Times = []\n",
    "aqs_Values = []\n",
    "aqs_Lats = []\n",
    "aqs_Longs = []\n",
    "\n",
    "aqs_Count = 12  # Here the EPA data catches up to the DSCOVR Data\n",
    "current = to_datetime(hr_aqs_time.index.values[aqs_Count])\n",
    "total_error = 0\n",
    "\n",
    "for i in range(len(times)):\n",
    "    x = times[i]\n",
    "    while current < x:\n",
    "        aqs_Count += 1\n",
    "        current = to_datetime(hr_aqs_time.index.values[aqs_Count])\n",
    "\n",
    "    prev = to_datetime(hr_aqs_time.index.values[aqs_Count] - 1)\n",
    "    prev_difference = x - prev\n",
    "    current_difference = current - x\n",
    "\n",
    "    if current_difference < prev_difference:\n",
    "        aqs_Times.append(to_datetime(hr_aqs_time.index.values[aqs_Count]))\n",
    "        aqs_Values.append(hr_aqs_time.values[aqs_Count])\n",
    "        aqs_Lats.append(hr_aqs_latitude.values[aqs_Count])\n",
    "        aqs_Longs.append(hr_aqs_longitude.values[aqs_Count])\n",
    "    else:\n",
    "        aqs_Times.append(to_datetime(hr_aqs_time.index.values[aqs_Count - 1]))\n",
    "        aqs_Values.append(hr_aqs_time.values[aqs_Count - 1])\n",
    "        aqs_Lats.append(hr_aqs_latitude.values[aqs_Count - 1])\n",
    "        aqs_Longs.append(hr_aqs_longitude.values[aqs_Count - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7574e2-0395-4b55-ab89-e2895f899712",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dscovr = get_normalized_data(timeseries_values)\n",
    "normalized_aqs = get_normalized_data(aqs_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219492a8-eaf4-448f-8872-013118034db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "ax.plot(times, normalized_dscovr, \":x\", markersize=7, color=color_green, label=\"DSCOVR EPIC\")\n",
    "ax.plot(aqs_Times, normalized_aqs, markersize=9, linewidth=4, color=color_purple, label=\"AQS\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(f\"{data_variable} @Hampton Roads, VA\\n Normalized DSCOVR EPIC and AQS\", fontsize=18)\n",
    "\n",
    "# Detailing the axis labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.savefig(\"DscovrFigures/02_UVAI_Initial_Comparison.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509a88b-dae1-4602-8b46-d2e1c31d16e7",
   "metadata": {},
   "source": [
    "## 5. RSIG and DSCOVR EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a8611-4311-4164-91b2-fc90328cb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "dscovr_before, dscovr_during, dscovr_after = get_date_splits(normalized_dscovr, times)\n",
    "aqs_before, aqs_during, aqs_after = get_date_splits(normalized_aqs, aqs_Times)\n",
    "\n",
    "# calculating RSME\n",
    "total_rsme = math.sqrt(mean_squared_error(normalized_aqs, normalized_dscovr))\n",
    "before_rsme = math.sqrt(mean_squared_error(aqs_before, dscovr_before))\n",
    "during_rsme = math.sqrt(mean_squared_error(aqs_during, dscovr_during))\n",
    "after_rsme = math.sqrt(mean_squared_error(aqs_after, dscovr_after))\n",
    "\n",
    "print(\"RSME Total:\\t\\t\\t\", total_rsme)\n",
    "print(\"RSME Before:\\t\\t\\t\", before_rsme)\n",
    "print(\"RSME During:\\t\\t\\t\", during_rsme)\n",
    "print(\"RSME After:\\t\\t\\t\", after_rsme)\n",
    "\n",
    "# Calculating Variance\n",
    "dscovr_xbar = sum(normalized_dscovr) / len(normalized_dscovr)\n",
    "var_dscovr = statistics.variance(normalized_dscovr, dscovr_xbar)\n",
    "\n",
    "print(\"\\nDSCOVR Normalized Variance:\\t\", var_dscovr)\n",
    "print(\"Variance Before:\\t\\t\", statistics.variance(dscovr_before, dscovr_xbar))\n",
    "print(\"Variance During:\\t\\t\", statistics.variance(dscovr_during, dscovr_xbar))\n",
    "print(\"Variance After:\\t\\t\\t\", statistics.variance(dscovr_after, dscovr_xbar))\n",
    "\n",
    "aqs_xbar = sum(normalized_aqs) / len(normalized_aqs)\n",
    "var_aqs = statistics.variance(normalized_aqs, aqs_xbar)\n",
    "\n",
    "print(\"\\nEPA Normalized Variance:\\t\", var_aqs)\n",
    "print(\"Variance Before:\\t\\t\", statistics.variance(aqs_before, aqs_xbar))\n",
    "print(\"Variance During:\\t\\t\", statistics.variance(aqs_during, aqs_xbar))\n",
    "print(\"Variance After:\\t\\t\\t\", statistics.variance(aqs_after, aqs_xbar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024d7a7-203b-4275-962d-d2aa90cc04f5",
   "metadata": {},
   "source": [
    "### Is Location the Issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e9b6f-b65b-4185-8628-9067913e8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the Coordinates into Latitude and Longitude Arrays for ease\n",
    "dscovr_lats = []\n",
    "dscovr_longs = []\n",
    "\n",
    "for val in nearestpt_coords:\n",
    "    dscovr_lats.append(val[1])\n",
    "    dscovr_longs.append(val[0])\n",
    "\n",
    "# Building the Projection\n",
    "my_projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 9), subplot_kw={\"projection\": my_projection})\n",
    "ax.set_facecolor(\"lightblue\")\n",
    "\n",
    "# Calling the County level data to build the outline\n",
    "county_shapefile = \"content/cartopy_data/cultural/ne_10m_admin_2_counties.shp\"\n",
    "counties = cfeature.ShapelyFeature(\n",
    "    Reader(county_shapefile).geometries(), my_projection, facecolor=\"white\", edgecolor=\"k\"\n",
    ")\n",
    "ax.add_feature(counties, linestyle=\":\", zorder=0)\n",
    "\n",
    "ax.set_extent([-76.7, -76, 36.8, 37.2], crs=my_projection)\n",
    "ax.add_feature(cfeature.STATES, linestyle=\"-\", linewidth=0.8, edgecolor=\"k\", zorder=1)\n",
    "\n",
    "# Putting the actual data onto the graph\n",
    "ax.scatter(aqs_Longs, aqs_Lats, marker=\"x\", s=150, color=color_purple, zorder=2, label=\"RSIG Data\")\n",
    "ax.scatter(\n",
    "    dscovr_longs,\n",
    "    dscovr_lats,\n",
    "    marker=\"o\",\n",
    "    s=30,\n",
    "    data=timeseries_values,\n",
    "    c=np.arange(len(dscovr_longs)),\n",
    "    cmap=muted_cmap,\n",
    "    zorder=3,\n",
    "    label=\"DSCOVR Data\",\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "# Adding the colorbar\n",
    "cb_handle = plt.colorbar(\n",
    "    _contourf,\n",
    "    orientation=\"horizontal\",\n",
    "    ticks=range(int(np.floor(vmin)), int(np.ceil(vmax + 1)), 1),\n",
    "    boundaries=level_boundaries,\n",
    "    values=(level_boundaries[:-1] + level_boundaries[1:]) / 2,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "cb_handle.ax.set_title(data_variable, fontsize=16)\n",
    "\n",
    "ax.set_title(\"All Data Point Locations in Hampton Roads\", fontsize=18)\n",
    "\n",
    "gl_handle = ax.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=[\"left\", \"bottom\"],\n",
    "    linewidth=0.8,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\":\",\n",
    ")\n",
    "\n",
    "# Detailing the axis labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "cb_handle.ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "gl_handle.xlabel_style, gl_handle.ylabel_style = {\"fontsize\": 16}, {\"fontsize\": 16}\n",
    "\n",
    "# plt.savefig(\"DscovrFigures/03_Locations_Graph\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbb845-9340-4aa5-805e-c2f3cd8b6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_val, p_val, std_err = stats.linregress(normalized_aqs, normalized_dscovr)\n",
    "\n",
    "print(\"Slope:\\t\\t\", slope)\n",
    "print(\"Intercept:\\t\", intercept)\n",
    "print(\"R Value:\\t\", r_val)\n",
    "print(\"P Value:\\t\", p_val)\n",
    "print(\"Standard Error:\\t\", std_err)\n",
    "\n",
    "plt.scatter(normalized_aqs, normalized_dscovr, color=color_green)\n",
    "plt.plot(normalized_aqs, slope * np.array(normalized_aqs) + intercept, color=color_purple)\n",
    "\n",
    "plt.title(\"AQS and DSCOVR EPIC Linear Regression\")\n",
    "plt.xlabel(\"Normalized AQS Data\")\n",
    "plt.ylabel(\"Normalized DSCOVR EPIC Data\")\n",
    "# plt.savefig(\"DscovrFigures/04_Initial_Linear_Regression\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ff5f3-54c7-4b62-b325-e3a5165161a6",
   "metadata": {},
   "source": [
    "## 6. Minimizing Error by shifting AQS time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4691dec-d65c-491b-81e7-6b85616fdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_timeshift_dscovr = []\n",
    "best_timeshift_aqs = []\n",
    "\n",
    "min_rsme = 1\n",
    "best_dilation = 0\n",
    "for i in range(int(len(normalized_aqs) / 3)):\n",
    "    timeshift_normalized_aqs = normalized_aqs[i : len(normalized_aqs)]\n",
    "    timeshift_normalized_dscovr = normalized_dscovr[0 : len(normalized_dscovr) - i]\n",
    "\n",
    "    curr_rsme = math.sqrt(mean_squared_error(timeshift_normalized_aqs, timeshift_normalized_dscovr))\n",
    "    if curr_rsme < min_rsme:\n",
    "        min_rsme = curr_rsme\n",
    "        best_dilation = i\n",
    "\n",
    "        best_timeshift_dscovr = timeshift_normalized_dscovr\n",
    "        best_timeshift_aqs = timeshift_normalized_aqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5e181-213a-4e14-8dd6-ecfea051e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "ax.plot(\n",
    "    times[0 : len(times) - best_dilation],\n",
    "    best_timeshift_dscovr,\n",
    "    \":x\",\n",
    "    markersize=7,\n",
    "    color=color_green,\n",
    "    label=\"DSCOVR EPIC\",\n",
    ")\n",
    "ax.plot(\n",
    "    times[0 : len(times) - best_dilation],\n",
    "    best_timeshift_aqs,\n",
    "    markersize=9,\n",
    "    linewidth=4,\n",
    "    color=color_purple,\n",
    "    label=\"AQS\",\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(\n",
    "    f\"{data_variable} @Hampton Roads, VA\\n Normalized DSCOVR EPIC and Timeshifted AQS\", fontsize=18\n",
    ")\n",
    "\n",
    "# Detailing the axis labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.savefig(\"DscovrFigures/05_UVAI_Timeshift_Comparison.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31652811-8e5d-4d9d-92aa-e54c430572a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dscovr_before, dscovr_during, dscovr_after = get_date_splits(\n",
    "    best_timeshift_dscovr, times[0 : len(times) - best_dilation]\n",
    ")\n",
    "aqs_before, aqs_during, aqs_after = get_date_splits(\n",
    "    normalized_aqs, times[0 : len(times) - best_dilation]\n",
    ")\n",
    "\n",
    "# calculating RSME\n",
    "total_rsme = math.sqrt(mean_squared_error(best_timeshift_aqs, best_timeshift_dscovr))\n",
    "before_rsme = math.sqrt(mean_squared_error(aqs_before, dscovr_before))\n",
    "during_rsme = math.sqrt(mean_squared_error(aqs_during, dscovr_during))\n",
    "after_rsme = math.sqrt(mean_squared_error(aqs_after, dscovr_after))\n",
    "\n",
    "print(\"RSME Total:\\t\\t\\t\", total_rsme)\n",
    "print(\"RSME Before:\\t\\t\\t\", before_rsme)\n",
    "print(\"RSME During:\\t\\t\\t\", during_rsme)\n",
    "print(\"RSME After:\\t\\t\\t\", after_rsme)\n",
    "\n",
    "# Calculating Variance\n",
    "timeshift_dscovr_xbar = sum(best_timeshift_dscovr) / len(best_timeshift_dscovr)\n",
    "timeshift_var_dscovr = statistics.variance(best_timeshift_dscovr, timeshift_dscovr_xbar)\n",
    "\n",
    "print(\"\\nDSCOVR Normalized Variance:\\t\", timeshift_var_dscovr)\n",
    "print(\"Variance Before:\\t\\t\", statistics.variance(dscovr_before, timeshift_dscovr_xbar))\n",
    "print(\"Variance During:\\t\\t\", statistics.variance(dscovr_during, timeshift_dscovr_xbar))\n",
    "print(\"Variance After:\\t\\t\\t\", statistics.variance(dscovr_after, timeshift_dscovr_xbar))\n",
    "\n",
    "timeshift_aqs_xbar = sum(best_timeshift_aqs) / len(best_timeshift_aqs)\n",
    "timeshift_var_aqs = statistics.variance(best_timeshift_aqs, timeshift_aqs_xbar)\n",
    "\n",
    "print(\"\\nAQS Normalized Variance:\\t\", timeshift_var_aqs)\n",
    "print(\"Variance Before:\\t\\t\", statistics.variance(aqs_before, timeshift_aqs_xbar))\n",
    "print(\"Variance During:\\t\\t\", statistics.variance(aqs_during, timeshift_aqs_xbar))\n",
    "print(\"Variance After:\\t\\t\\t\", statistics.variance(aqs_after, timeshift_aqs_xbar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec19fe0-033f-4397-84ef-d3eeae9095e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_val, p_val, std_err = stats.linregress(\n",
    "    best_timeshift_aqs, best_timeshift_dscovr\n",
    ")\n",
    "\n",
    "print(\"Slope:\\t\\t\", slope)\n",
    "print(\"Intercept:\\t\", intercept)\n",
    "print(\"R Value:\\t\", r_val)\n",
    "print(\"P Value:\\t\", p_val)\n",
    "print(\"Standard Error:\\t\", std_err)\n",
    "\n",
    "plt.scatter(best_timeshift_aqs, best_timeshift_dscovr, color=color_green)\n",
    "plt.plot(normalized_aqs, slope * np.array(normalized_aqs) + intercept, color=color_purple)\n",
    "\n",
    "plt.title(\"DSCOVR EPIC and Timeshift AQS Linear Regression\")\n",
    "plt.xlabel(\"Normalized Timeshifted AQS Data\")\n",
    "plt.ylabel(\"Normalized DSCOVR EPIC Data\")\n",
    "# plt.savefig(\"DscovrFigures/06_Timeshift_Linear_Regression\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb2bb5-0db3-40ff-9786-84dc8e2a2a0f",
   "metadata": {},
   "source": [
    "END of Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-dscovr-baeza-notebook",
   "language": "python",
   "name": "temp-dscovr-baeza-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
